\documentclass{article}
\pagestyle{plain}
\usepackage[utf8]{inputenc}
\usepackage{tabularx}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage[skip=-2pt]{subcaption}
\usepackage[skip=5pt]{caption}
\usepackage[pdftex,
            pdfauthor=Jimmy Aguilar Mena,
            pdftitle={OmpSs-2@Cluster: Distributed memory execution of nested OpenMP-style tasks},
            pdfsubject={Europar-2022},
            pdfkeywords={OmpSs-2,Task,Cluster},
            pdfproducer={Latex with hyperref},
            pdfcreator={pdflatex}]{hyperref}
\usepackage{booktabs}

\usepackage{listings}
\lstset{numbers=none,basicstyle=\scriptsize\ttfamily,language=bash,frame=single}

\usepackage{sansmath}

\newcommand{\prag}[1]{\textcolor{blue}{#1}}
\newcommand{\paul}[1]{\textcolor{purple}{#1}}

\usepackage[T1]{fontenc}
\usepackage{multirow}

\newcommand{\code}[1]{\texttt{#1}}

\begin{document}

\title{OmpSs-2@Cluster: Distributed memory execution of nested OpenMP-style tasks}

\author{Jimmy Aguilar Mena, Omar Shaaban, Vicen\c{c} Beltran,\\
  Paul Carpenter, Eduard Ayguade, Jesus Labarta}
\date{Artifact}
%\institute{Barcelona Supercomputing Center}

\maketitle

This artifact explains how to reproduce the results from our paper.
It comprises this set of instructions and a script, \code{artifact.sh},
which automates all the necessary steps. The script downloads and
installs the Nanos6 runtime and the Mercurium source-to-source
compiler. Then it downloads and runs a small subset of the
benchmarks. The entire installation requires $<$2\,Gb of disk space.

\section{System, Dependencies and Environment}

\subsection{System}

Memory overcommitment must be enabled. On GNU/Linux, the status can be
checked using:

\begin{lstlisting}
cat /proc/sys/vm/overcommit_memory
\end{lstlisting}

If the value is not zero, it should be set to zero using:

\begin{lstlisting}
echo 0 > /proc/sys/vm/overcommit_memory
\end{lstlisting}

Address Space Layout Randomization~(ASLR) must be disabled.  The
runtime is usually able to disable ASLR itself, if needed. But some
systems restrict some users to do so, if that's tour case it may be
necessary to explicitly disable ASLR using:

\begin{lstlisting}
echo 0 > /proc/sys/kernel/randomize_va_space
\end{lstlisting}

\subsection{Dependencies}

The software dependencies are:

\begin{enumerate}
    \item automake, autoconf, libtool, pkg-config and make.

    \item C and C++ compiler:  gcc\,7.2.0 and icpc\,2018.1 are known to work without
        issues.\footnote{To build the tests Mercurium provides different executables;
        in order to use gcc/g++ (mcc/mcxx) or icc (imc/imcxx).}

    \item bison, gperf, libsqlite3 and flex.

    \item boost >= 1.59: In our experiments we use boost\,1.64.0.

    \item hwloc: If you use OpenMPI then it will be a dependency as there are some
        version constraints between OpenMPI and hwloc. In our experiments we use
        hwloc 1.11.8.

    \item MPI library with \code{MPI\_THREAD\_MULTIPLE} support. Our communication
        uses Intel MPI\,2018.4 over 100\,Gb/s Intel OmniPath, with an HFI Silicon
        100 series PCIe adaptor.  IMPI can be freely downloaded from the Intel
        website \href{https://www.intel.com/content/www/us/en/developer/tools/oneapi/mpi-library.html}{IMPI};
        but the runtime also works with OpenMPI and MPICH.\footnote{The final results may strongly depend of the MPI multithreaded
        support and optimization; we observed degraded performance results
        when using OpenMPI.}

    \item cmake >= 3.10 and BLAS/LAPACK: to build the optimized benchmarks.
        We use Intel MKL in our experiments. It can be freely
        downloaded from the Intel web site:
        \href{https://www.intel.com/content/www/us/en/developer/tools/oneapi/onemkl-download.html}{oneMKL}

    \item python3, matplotlib and pandas: to process the outputs and generate the plots.

\end{enumerate}

\section{Build and install}

\subsection{Automated approach using the script}

We provide the script \code{artifact.sh} to automatize the steps
explained in this section. To run it you need an interactive session
with internet access (to clone the repositories) where mpi can run at
least on two nodes because this is a scalability experiment. For more
details see section\,\ref{SBS}.

\subsection{Nanos6 and Mercurium manual build and installation}\label{manuallib}

The nanos6 basic installation instructions are on:
\href{https://github.com/bsc-pm/nanos6-cluster}{nanos6-cluster} the
README.md file explains the dependencies and some other features and
it is updated with every release. To download build and install we can
use the release
\href{https://github.com/bsc-pm/ompss-2-cluster-releases}{2022.02}.

\begin{lstlisting}
git clone --branch 2022.02 \
          https://github.com/bsc-pm/ompss-2-cluster-releases
cd ompss-2-cluster-releases
export STARTDIR=${PWD}
git submodule init
git submodule update --depth 1

export NANOS6_HOME=${STARTDIR}/nanos6-cluster-install
cd ${STARTDIR}/nanos6-cluster
sed -i '12i#include <cstddef>' src/memory/AddressSpace.hpp
autoreconf -vif
./configure --prefix=${NANOS6_HOME} \
            --enable-cluster --enable-execution-workflow \
            --disable-lint-instrumentation \
            --disable-ctf-instrumentation \
            --disable-graph-instrumentation \
            --disable-stats-instrumentation \
            --disable-extrae-instrumentation \
            --disable-verbose-instrumentation
make install
\end{lstlisting}

As usual the options CC=gcc CXX=g++ can be used to specify the C and
C++ compilers --with-boost= to specify BOOST path in case it is not in
the default path.

The compiler for Mercurium is on:
\href{https://github.com/bsc-pm/mcxx}{mcxx}. To build it we need the
following steps.

\begin{lstlisting}
echo "# Starting Mercurium installation."
cd ${STARTDIR}/mcxx
autoreconf -vif
export MERCURIUM_HOME=${STARTDIR}/mcxx-install
./configure --prefix=${MERCURIUM_HOME} \
            --with-nanos6=${NANOS6_HOME} \
            --enable-ompss-2
make install
export PATH=${MERCURIUM_HOME}/bin:${PATH}
\end{lstlisting}

\subsection{Benchmark build and test}\label{manualbench}

The MPI benchmarks are in different repositories so we need to build
them separately. This is a requirement because cmake does not allow to use
different compilers in the same project; and the MPI benchmarks does
not need to use Mercurium; but be careful to build both with the same
compiler to get consistent results.

For the OmpSs-2@Cluster benchmarks:

\begin{lstlisting}
cd ${STARTDIR}
git clone --depth=1 --branch=europar \
          https://github.com/Ergus/nanos-cluster-benchmarks
mkdir nanos-cluster-benchmarks/build
cd nanos-cluster-benchmarks/build
cmake -DCMAKE_BUILD_TYPE=Release ..
make
# Don't forget this
export NANOS6_CONFIG=${STARTDIR}/nanos-cluster-benchmarks/build/nanos6.toml
\end{lstlisting}

For the MPI benchmarks:

\begin{lstlisting}
cd ${STARTDIR}
git clone --depth=1 --branch=europar --recursive \
          https://github.com/Ergus/MPI_Benchmarks
mkdir MPI_Benchmarks/build
cd MPI_Benchmarks/build
cmake -DCMAKE_BUILD_TYPE=Release ..
make
\end{lstlisting}

By default cmake will try to use the gcc compiler; to use icc it is
possible to pass the option \code{-DCCOMPILER=intel} in the cmake
line.

By default cmake will try to build with the system's LAPACK library
following the cmake policy to find it:
\href{https://cmake.org/cmake/help/latest/module/FindLAPACK.html}{FindLAPACK}
\href{https://cmake.org/cmake/help/latest/module/FindBLAS.html}{FindBLAS}

We added an special option \code{-DWITH\_MKL=true} to simplify forcing
to use Intel MKL (recommended).

If everything was right all the tests may be build now with the
submission scripts in their sub-directories.

\subsection{Running benchmarks}

We provide two sets of scripts to run the benchmarks:
\code{interactive\_dim.sh} and \code{submitter\_dim.sh}. These scripts
are copied by cmake automatically into every benchmark build
directory.

The following instructions apply to MPI and OmpSs-2@Cluster
benchmarks.  They are executed in the same way, the command line
arguments, outputs and submission scripts are the same.

The script \code{artifact.sh} tries to execute using
\code{interactive\_dim.sh}.

\subsubsection{OmpSs benchmarks (interactively)}\label{INTERACTIVE}
To execute a benchmark in an interactive session you just need to go
to the build directory and execute it as a normal MPI aplication, the
OmpSs-2@Cluster runtime relies on your MPI to distribute the processes
and communicate.

\begin{lstlisting}
cd ${STARTDIR}/nanos-cluster-benchmarks/build/jacobi_ompss2
mpirun -np $NP ./jacobi_task_fetchall_blas_ompss2 $DIM $BS $ITS
\end{lstlisting}

Where \code{\$NP} is the number of processes to use, \code{\$DIM} is the
problem dimension, \code{\$BS} is the block size and \code{\$ITS} is the
number of iterations to execute.

One example execution and its output may be:
\begin{lstlisting}
> mpirun -np 2 ./jacobi_task_nofetch_blas_ompss2 1024 16 4

# Initializing data
# Starting algorithm
# jacobi tasks FETCHTASK=0
# Finished algorithm...
Executable: "./jacobi_task_nofetch_blas_ompss2"
Rows: 1024
Tasksize: 16
Iterations: 4
Print: 0
worldsize: 2
cpu_count: 24
namespace_enabled: 1
nanos6_version: "2.5.1 2022-05-08 21:40:06 +0200 2a61a546"
Total_time: 9.47129e+07
Algorithm_time: 7.96941e+07
\end{lstlisting}

If you see a duplicated output it means that the cluster mode was not
properly set in nanos6, so probably you are setting a wrong value for
\code{NANOS6\_CONFIG} or the options \code{--enable-cluster} and
\code{--enable-execution-workflow} were missing when configuring the
OmpSs-2@Cluster runtime library nanos6.

The script \code{interactive\_dim.sh} to execute multiple benchmarks
interactively. The script is automatically copied into the build
directories and its use is very simple:

\begin{lstlisting}
./interactive_dim.sh -N 1,2 -R 1 -D 32768 -B 64 -I 1 exe1 exe2 exe3
\end{lstlisting}

This will execute mpirun with -n \code{1} and \code{2} nodes (-N) for
the executable files: \code{exe1}, \code{exe2} and
\code{exe3}. Repeating the execution \code{3} times (-R). With a
problem dimension of \code{32768} (-D), block size \code{64} (-B) and
for \code{5} iterations (-I).

If you are in a Slurm system with an interactive session you could
consider to substitute \code{mpirun} with \code{srun} inside the
script.

\subsubsection{OmpSs benchmarks (Slurm cluster submit)}\label{SUBMIT}

If your system uses Slurm and you are not in an interactive session,
then you can execute \code{submitter\_dim.sh} instead of
\code{interactive\_dim.sh}.

The submit scripts accept extra command line arguments to specify
partitions, walltime, and other Slurm-specific options. You can read
that directly in the file.

For example:

\begin{lstlisting}
./submitter_dim.sh -N 1,2 -R 3 -C 24 -D 32768 -B 32,64 -I 5 \
	-o results exe1 exe2 exe3
\end{lstlisting}

This will submit 3 jobs with sbatch for \code{1} and \code{2} nodes
(-N) respectively. That will run the executable files: \code{exe1},
\code{exe2} and \code{exe3} with srun \code{3} times each (-R) in a
loop with \code{24} cores per process (-C) and will create a directory
\code{results} (-o) to save all the outputs. With a dimension of
\code{32768} (-D), block sizes of \code{32} and \code{64} (-B) and for
\code{5} iterations (-I).

The script also creates a file inside results named
\code{results/submit.log} with information about the submitted jobs in
case you need to re-check them latter.

\section{Step by step instructions}\label{SBS}

The script \code{artifact.sh} builds and downloads everything in the
caller directory (\code{cwd}). And it creates a new file
\code{nanos6\_automatic\_build.log} that can be used to diagnose
issues.

The outputs are added in the same directory in different files that we
can process individually with the provided Python script
\code{process\_dim.py}.

The full download, build and installation may take about 30
minutes. The benchmarks execution time depends of the problem size
(-D), Iterations (-I) and executions (-R).

\begin{lstlisting}
chmod a+x artifact.sh
chmod a+x process_dim.sh
./artifact.sh
\end{lstlisting}

It will try to download the libraries, and benchmark, build them with
gcc and the system LAPACK; then execute and create the plots. See the
instructions in (\ref{manualbench}) to build with icc and MKL if they
are available in your system. The current problem sizes are very big;
for quick tests reduce them as explained bellow.

We evaluate OmpSs-2@Cluster on MareNostrum\,4. Each node has two
24-core Intel Xeon Platinum 8160 CPUs at 2.10\,GHz, for a total of 48
cores per node.  Each socket has a shared 32\,MB L3 cache.  The HPL
Rmax performance equates to 1.01\,TF per socket.  Communication uses
Intel MPI\,2018.4 over 100\,Gb/s Intel OmniPath, with an HFI Silicon
100 series PCIe adaptor.  All the kernels use the same standard BLAS
functions from Intel MKL\,2018.4.

The script contains the same problem sizes and number of iterations
used in the paper. Such executions may take several hours to complete
all the experiments, especially on smaller systems. For quick tests we
recommend to divide by 4 every problem size (-D) and block sizes (-B);
but also reduce the number of executions (-R) and iterations (-I) to 1
or 3. For simplicity the code demands that -D may be divisible by all
-N values and -B. The same command line options can be used either
with the interactive (\ref{INTERACTIVE}) or the submit \ref{SUBMIT}
scripts. The current number of nodes to run the benchmarks is 1 and 2
(-N 1,2).

\begin{lstlisting}
cd ${STARTDIR}/nanos-cluster-benchmarks/build/matmul_matvec_ompss2

cp ${STARTDIR}/MPI_Benchmarks/build/matmul_matvec_mpi/matvec_parallelfor_blas_mpi .
./interactive_dim.sh -N 1,2 -R 3 -D $((16*1024)) -B 64 -I 3 \
	matvec_strong_flat_task_node_blas_ompss2 \
	matvec_weak_fetchall_task_node_blas_ompss2 \
	matvec_parallelfor_blas_mpi | tee ${STARTDIR}/output_matvec.txt

cp ${STARTDIR}/MPI_Benchmarks/build/matmul_matvec_mpi/matmul_parallelfor_blas_mpi .
./interactive_dim.sh -N 1,2 -R 3 -D $((4*1024)) -B 8 -I 1 \
	matmul_strong_nested_task_node_blas_ompss2 \
	matmul_weak_fetchall_task_node_blas_ompss2 \
	matmul_parallelfor_blas_mpi | tee ${STARTDIR}/output_matmul.txt

cd ${STARTDIR}/nanos-cluster-benchmarks/build/jacobi_ompss2
cp ${STARTDIR}/MPI_Benchmarks/build/jacobi_mpi/jacobi_parallelfor_nop2p_blas_mpi .
./interactive_dim.sh -N 1,2 -R 3 -D $((16*1024)) -B 256 -I 3 \
	jacobi_task_fetchall_blas_ompss2 \
	jacobi_taskfor_blas_ompss2 \
	jacobi_parallelfor_nop2p_blas_mpi | tee ${STARTDIR}/output_jacobi.txt

cd ${STARTDIR}/nanos-cluster-benchmarks/build/cholesky_fare_ompss2
cp ${STARTDIR}/MPI_Benchmarks/build/cholesky_mpi/cholesky_omp_mpi .
./interactive_dim.sh -N 1,2 -R 3 -D $((16*1024)) -B 128 -I 0 \
	cholesky_fare_strong_ompss2 \
	cholesky_fare_taskfor_ompss2 \
	cholesky_omp_mpi | tee ${STARTDIR}/output_cholesky.txt
\end{lstlisting}

The previous lines should produce 4 output files with the benchmarks
outputs: \code{output\_<benchmark>.txt}.

To process these files and get equivalent strong scalability than the
Fig\,5 in the paper we provide a Python script to process the output,
calculate the FLOPS and generate the plots (\code{artifact.sh} tries to
call it directly):

\begin{lstlisting}
cd ${STARTDIR}
./process_dim.py output_*.txt
\end{lstlisting}

The script will create a plot with the same basename, but with a png
extension. It will also print the numeric results to stdout.

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
