\documentclass{article}
\pagestyle{plain}
\usepackage[utf8]{inputenc}
\usepackage{tabularx}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage[skip=-2pt]{subcaption}
\usepackage[skip=5pt]{caption}
\usepackage[pdftex,
            pdfauthor=Jimmy Aguilar Mena,
            pdftitle={OmpSs-2@Cluster: Distributed memory execution of nested OpenMP-style tasks},
            pdfsubject={Europar-2022},
            pdfkeywords={OmpSs-2,Task,Cluster},
            pdfproducer={Latex with hyperref},
            pdfcreator={pdflatex}]{hyperref}
\usepackage{booktabs}

\usepackage{listings}
\lstset{numbers=left,basicstyle=\scriptsize,language=bash}

\usepackage{sansmath}

\newcommand{\prag}[1]{\textcolor{blue}{#1}}

\usepackage[T1]{fontenc}
\usepackage{multirow}

\newcommand{\code}[1]{\texttt{#1}}

\begin{document}

\title{OmpSs-2@Cluster: Distributed memory execution of nested OpenMP-style tasks}

% \author{
%   Jimmy Aguilar Mena\inst{1} \and %\orcidID{0000-0001-6802-2247} \and
%   Omar Shaaban\inst{1} \and %\ordicID{0000-0003-4410-5317} \and
%   Vicen\c{c} Beltran\inst{1} \and %\orcidID{0000-0002-3580-9630} \and
%   Paul Carpenter\inst{1} \and %\orcidID{0000-0002-9392-0521} \and
%   Eduard Ayguade\inst{1} \and %\orcidID{0000-0002-5146-103X} \and
%   Jesus Labarta\inst{1} %\orcidID{0000-0002-7489-4727}
% }
%\institute{Barcelona Supercomputing Center}

\maketitle

This paper it about features implemented in OmpSs-2@Cluster, so the
first step to reproduce the results is to install Nanos6 and the
source to source compiler Mercurium to create a functional setup. Then
we need to download and build the benchmarks used in the article to
finally execute them a process the results.

The script provided in this artifact: \code{build.sh} attempts to
perform those actions in order.

\section{System, Dependencies and Environment}

In general any GNU/Linux system allowing memory over-subscription and
disable address space randomization may work with no problem.

To build execute and create all the graphs we have these dependencies:

\begin{enumerate}
    \item automake, autoconf, libtool, pkg-config and make.

    \item C and C++ compiler: to build the nanos6, Mercurium and the benchmarks;
        we have tested gcc\,7.2.0 and icpc\,2018.1 and they both work without
        issues.
        \footnote{To build the tests Mercurium provides different executables;
        in order to use gcc/g++ (mcc/mcxx) or icc (imc/imcxx).}

    \item bison, gperf, libsqlite3 and flex: to build mercurium

    \item boost >= 1.59: In our experiments we use boost\,1.64.0.

    \item hwloc: If you use OpenMPI then it will be a dependency as there are some
        version constrains between OpenMPI and hwloc. In our experiments we use
        version 1.11.8.

    \item MPI library with \code{MPI\_THREAD\_MULTIPLE} support. Our communication
        uses Intel MPI\,2018.4 over 100\,Gb/s Intel OmniPath, with an HFI Silicon
        100 series PCIe adaptor.  IMPI can be freely downloaded from the Intel
        website \href{https://www.intel.com/content/www/us/en/developer/tools/oneapi/mpi-library.html}{IMPI};
        but the runtime works with OpenMPI and MPICH as well.
        \footnote{The final results may strongly depend of the MPI multithread
        support and optimization; we observed degraded performance results
        when using OpenMPI.}

    \item cmake >= 3.10 and Blas/Lapack: to build the optimized benchmarks.
        We use Intel MKL in our experiments. It can be freely
        downloaded from the intel web site:
        \href{https://www.intel.com/content/www/us/en/developer/tools/oneapi/onemkl-download.html}{oneMKL}

    \item python3, matplotlib and pandas: to process the outputs and generate the graphics.

\end{enumerate}

\section{Build and install}

We provide the script \code{build.sh} to automatize the steps
explained in this section.

The script contains the same problem sizes and number of
iterations used in the document. Such executions make take several
hours to complete all the experiments, specially in smaller
systems. For quick tests we recommend to divide by 4 every problem
size (-D) and reduce the number of executions (-R) to 1-3.

The script \code{build.sh} builds and downloads everything in the
caller directory (\code{cwd}). And creates a new file
\code{nanos6\_automatic\_build.log} that we can used to check and
solve issues.

The outputs are added in the same directory in different files that we
can process individually with the provided python script
\code{process\_dim.py}.

The full download, build and installation may take about 30
minutes. The benchmarks execution time depends of the problem size
(-D), Iterations (-I) and executions (-R).

\subsection{Nanos6 and Mercurium manual build and installation}

The nanos6 basic installation instructions are on:
\href{https://github.com/bsc-pm/nanos6-cluster}{nanos6-cluster} the
readme file explains the dependencies and some other features and it
is updated with every release. To download build and install we can
use the release
\href{https://github.com/bsc-pm/ompss-2-cluster-releases}{2022.02}.

\begin{lstlisting}
git clone --branch 2022.02 https://github.com/bsc-pm/ompss-2-cluster-releases
cd ompss-2-cluster-releases
export STARTDIR=${PWD}
git submodule init
git submodule update --depth 1

export NANOS6_HOME=${STARTDIR}/nanos6-cluster-install
cd ${STARTDIR}/nanos6-cluster
sed -i '12i#include <cstddef>' src/memory/AddressSpace.hpp
autoreconf -vif
./configure --prefix=${NANOS6_HOME} \
            --enable-cluster --enable-execution-workflow \
            --disable-lint-instrumentation --disable-ctf-instrumentation \
            --disable-graph-instrumentation --disable-stats-instrumentation \
            --disable-extrae-instrumentation --disable-verbose-instrumentation
make install
\end{lstlisting}

As usual the options CC=gcc CXX=g++ can be used to specify the C and
C++ compilers --with-boost= to specify BOOST path in case it is not in
the default path.

The compiler for mercurium is on:
\href{https://github.com/bsc-pm/mcxx}{mcxx}. To build it we need the
following steps.

\begin{lstlisting}
echo "# Starting Mercurium installation."
cd ${STARTDIR}/mcxx
autoreconf -vif
export MERCURIUM_HOME=${STARTDIR}/mcxx-install
./configure --prefix=${MERCURIUM_HOME} --with-nanos6=${NANOS6_HOME} \
            --enable-ompss-2
make install
export PATH=${MERCURIUM_HOME}/bin:${PATH}
\end{lstlisting}

\subsection{Benchmark build and test}

The MPI benchmarks are in different repositories so we need to build
them appart. This is a requirement because cmake does not allow to use
different compilers in the same project; and the MPI benchmarks does
not need to use mercurium; but be careful to build both with the same
compiler to get consistent results.

For the OmpSs-2@Cluster benchmarks:

\begin{lstlisting}
cd ${STARTDIR}
git clone --depth=1 --branch=europar https://github.com/Ergus/nanos-cluster-benchmarks
mkdir nanos-cluster-benchmarks/build
cd nanos-cluster-benchmarks/build
cmake -DCMAKE_BUILD_TYPE=Release ..
make
export NANOS6_CONFIG=${STARTDIR}/nanos-cluster-benchmarks/build/nanos6.toml
\end{lstlisting}

For the MPI benchmarks:

\begin{lstlisting}
cd ${STARTDIR}
git clone --depth=1 --branch=europar --recursive https://github.com/Ergus/MPI_Benchmarks
mkdir MPI_Benchmarks/build
cd MPI_Benchmarks/build
cmake -DCMAKE_BUILD_TYPE=Release ..
make
\end{lstlisting}

By default cmake will try to use the gcc compiler; to use icc it is
possible to pass the option \code{-DCCOMPILER=intel} in the cmake
line.

By default cmake will try to build with the system's Lapack library
following the cmake policy to find it:
\href{https://cmake.org/cmake/help/latest/module/FindLAPACK.html}{FindLAPACK}
\href{https://cmake.org/cmake/help/latest/module/FindBLAS.html}{FindBLAS}

We added an special option \code{-DWITH\_MKL=true} to simplify forcing
to use Intel MKL (recommended).

If everything was right all the tests may be build now with the
submission scripts in their sub-directories. In an interactive session
you may try: \code{ctest} from within the benchmark build directories
to check they can run in the system.

\subsection{Running benchmarks}

We provide two sets of scripts to run the benchmarks:
\code{interactive\_dim.sh} and \code{submiter\_dim.sh}.

The following instructions apply to MPI and OmpSs-2@Cluster
benchmarks.  They are executed in the same way, the command line
arguments, outputs and submission scripts are the same.

The script \code{build.sh} tries to execute using
\code{interactive.sh}.

\subsubsection{OmpSs benchmarks (interactively)}
To execute a benchmark in an interactive session you just need to go
to the build directory and execute it as a normal MPI aplication, the
OmpSs-2@Cluster runtime relies on your MPI to distribute the processes
and communicate.

\begin{lstlisting}
cd ${STARTDIR}/nanos-cluster-benchmarks/build/jacobi_ompss2
mpirun -np $NP ./jacobi_task_fetchall_blas_ompss2 $DIM $BS $ITS
\end{lstlisting}

Where \code{\$NP} is the number of processes to use, \code{\$DIM} the
problem dimension, \code{\$BS} the block size and \code{\$ITS} the
number of iterations to execute.

One example execution and its output may be:
\begin{lstlisting}
> mpirun -np 2 ./jacobi_task_nofetch_blas_ompss2 1024 16 4

# Initializing data
# Starting algorithm
# jacobi tasks FETCHTASK=0
# Finished algorithm...
Executable: "./jacobi_task_nofetch_blas_ompss2"
Rows: 1024
Tasksize: 16
Iterations: 4
Print: 0
worldsize: 2
cpu_count: 24
namespace_enabled: 1
nanos6_version: "2.5.1 2022-05-08 21:40:06 +0200 2a61a546"
Total_time: 9.47129e+07
Algorithm_time: 7.96941e+07
\end{lstlisting}

If you see a duplicated output it means that the cluster mode was not
properly set in nanos6, so probably you are setting a wrong value for
\code{NANOS6\_CONFIG} or the options \code{--enable-cluster} and
\code{--enable-execution-workflow} where missing when configuring the
OmpSs-2@Cluster runtime library nanos6.

The script \code{interactive\_dim.sh} to execute multiple benchmarks
interactively. The script is automatically copied into the build
directories and its use is very simple:

\begin{lstlisting}
./interactive_dim.sh -N 1,2 -R 1 -D 32768 -B 64 -I 1 exe1 exe2 exe3
\end{lstlisting}

This will execute mpirun with -n \code{1} and \code{2} nodes (-N) for
the executable files: \code{exe1}, \code{exe2} and
\code{exe3}. Repeating the execution \code{3} times (-R). With a
problem dimension of \code{32768} (-D), block size \code{64} (-B) and
for \code{5} iterations (-I).

If you are in a slurm system with an interactive session you could
consider to substitute \code{mpirun} with \code{srun} inside the
script.

\subsubsection{OmpSs benchmarks (Slurm cluster submit)}

If your system uses slurm and you are not in an interactive session,
then you can use a \code{submiter\_dim.sh} in the same way than
\code{interactive\_dim.sh}. 

The submit scripts accepts extra command line arguments to specify
partitions, walltime, and other slurm specific options. You can read
that directly in the file.

For example:

\begin{lstlisting}
./submiter_dim.sh -N 1,2 -R 3 -C 24 -D 32768 -B 32,64 -I 5 -o results exe1 exe2 exe3
\end{lstlisting}

This will submit 3 jobs with sbatch for \code{1} and \code{2} nodes
(-N) respectively. That will run the executable files: \code{exe1},
\code{exe2} and \code{exe3} with srun \code{3} times each (-R) in a
loop with \code{24} cores per process (-C) and will create a directory
\code{results} (-o) to save all the outputs. With a dimension of
\code{32768} (-D), block sizes of \code{32} and \code{64} (-B) and for
\code{5} iterations (-I).

The script also creates a file inside results named
\code{results/submit.log} with information about the submitted jobs in
case you need to re-check them latter.

\section{Step by step instructions}

We evaluate OmpSs-2@Cluster on
MareNostrum\,4~\cite{MareNostrum4}. Each node has two 24-core Intel
Xeon Platinum 8160 CPUs at 2.10\,GHz, for a total of 48 cores per
node.  Each socket has a shared 32\,MB L3 cache.  The HPL Rmax
performance equates to 1.01\,TF per socket.  Communication uses Intel
MPI\,2018.4 over 100\,Gb/s Intel OmniPath, with an HFI Silicon 100
series PCIe adaptor.  All the kernels use the same code and same
standard BLAS functions from Intel MKL\,2018.4.

To partially reproduce some of the results in the paper, the execution
may require a big setup (up to 16 nodes and 48 cores per node in two
numa nodes and enough ram to allocate the matrices). You can reduce
the problem size (-D), the number of iterations (-I) and the time the
experiments are repeated (-R) to reduce the execution time. This
similar commands can be used either with the interactive or the submit
scripts.

To execute a quick test you could reduce the problem size (-D)
dividing the provided values bellow by 4 and reduce the number of
executions (-R) to 1 or 3 and the iterations to 1 or 2. \footnote{In
  cholesky the -I parameter needs to be always zero.}

\begin{lstlisting}
cd ${STARTDIR}/nanos-cluster-benchmarks/build/matmul_matvec_ompss2

cp ${STARTDIR}/MPI_Benchmarks/build/matmul_matvec_mpi/matvec_parallelfor_blas_mpi .
./interactive_dim.sh -N 1,2 -R 10 -D 65536 -B 256 -I 5 \
	matvec_strong_flat_task_node_blas_ompss2 \
	matvec_weak_fetchall_task_node_blas_ompss2 \
	matvec_parallelfor_blas_mpi | tee ${STARTDIR}/output_matvec.txt

cp ${STARTDIR}/MPI_Benchmarks/build/matmul_matvec_mpi/matmul_parallelfor_blas_mpi .
./interactive_dim.sh -N 1,2 -R 10 -D 16384 -B 16 -I 1 \
	matmul_strong_nested_task_node_blas_ompss2 \
	matmul_weak_fetchall_task_node_blas_ompss2 \
	matmul_parallelfor_blas_mpi | tee ${STARTDIR}/output_matmul.txt

cd ${STARTDIR}/nanos-cluster-benchmarks/build/jacobi_ompss2
cp ${STARTDIR}/MPI_Benchmarks/build/jacobi_mpi/jacobi_parallelfor_nop2p_blas_mpi .
./interactive_dim.sh -N 1,2 -R 10 -D 65536 -B 256 -I 5 \
	jacobi_task_fetchall_blas_ompss2 \
	jacobi_taskfor_blas_ompss2 \
	jacobi_parallelfor_nop2p_blas_mpi | tee ${STARTDIR}/output_jacobi.txt

cd ${STARTDIR}/nanos-cluster-benchmarks/build/cholesky_fare_ompss2
cp ${STARTDIR}/MPI_Benchmarks/build/cholesky_mpi/cholesky_omp_mpi .
./interactive_dim.sh -N 1,2 -R 10 -D 65536 -B 512 -I 0 \
	cholesky_fare_strong_ompss2 \
	cholesky_fare_taskfor_ompss2 \
	cholesky_omp_mpi | tee ${STARTDIR}/output_cholesky.txt
\end{lstlisting}

The previous lines should produce 4 output files with the numeric
results: \code{output\_matvec.txt}, \code{output\_matmul.txt}
\code{output\_jacobi.txt} and \code{output\_cholesky.txt}.

To process these files and get equivalent strong scalability than the
Fig\,5 in the paper we provide a python script to process the output,
calculate the FLOPS and generate the plots:

\begin{lstlisting}
cd ${STARTDIR}
./process_dim.py output_*.txt
\end{lstlisting}

The script will create a plot with the same basename, but with png
extension. And will print the numeric results to stdout.

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
